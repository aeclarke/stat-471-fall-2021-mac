---
title: 'STAT 471: Homework 2'
author: 'Ashley Clarke'
date: 'Due: October 4, 2021 at 11:59pm'
output:
  bookdown::pdf_document2:
    number_sections: yes
    toc: yes
    toc_depth: '2'
  html_document:
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: no
    toc_depth: 4
    toc_float: yes
urlcolor: blue
---

```{r setup, include=FALSE}
options(scipen = 0, digits = 3)  # controls number of significant digits printed
```

\newpage

# Instructions {-}

## Setup {-} 

Pull the latest version of this assignment from Github and set your working directory to `stat-471-fall-2021/` `homework/homework-2`. Consult the [getting started guide](https://github.com/Katsevich-Teaching/stat-471-fall-2021/blob/main/getting-started/getting-started.pdf) if you need to brush up on `R` or `Git`.

## Collaboration {-}

The collaboration policy is as stated on the Syllabus:

>"Students are permitted to work together on homework assignments, but solutions must be written up and submitted individually. Students must disclose any sources of assistance they received; furthermore, they are prohibited from verbatim copying from any source and from consulting solutions to problems that may be available online and/or from past iterations of the course."

In accordance with this policy, 

*Please list anyone you discussed this homework with:* 
Zach Bradlow, Paul Heysch de la Borde, and Sarah Hu

*Please list what external references you consulted (e.g. articles, books, or websites):*
- https://liangf.wordpress.com/2019/08/18/degree-of-freedom-for-knn/
- https://medium.com/30-days-of-machine-learning/day-3-k-nearest-neighbors-and-bias-variance-tradeoff-75f84d515bdb

## Writeup {-}

Use this document as a starting point for your writeup, adding your solutions after "**Solution**". Add your R code using code chunks and add your text answers using **bold text**. Consult the [preparing reports guide](https://github.com/Katsevich-Teaching/stat-471-fall-2021/blob/main/getting-started/preparing-reports.pdf) for guidance on compilation, creation of figures and tables, and presentation quality. 

## Programming {-}

The `tidyverse` paradigm for data wrangling, manipulation, and visualization is strongly encouraged, but points will not be deducted for using base \texttt{R}. 


We'll need to use the following `R` packages:
```{r, message = FALSE}
library(tidyverse)  # tidyverse
library(kableExtra) # for printing tables
library(cowplot)    # for side by side plots
library(FNN)        # for K-nearest-neighbors regression
```

We'll also need the `cross_validate_spline` function from Unit 2 Lecture 3:
```{r}
source("../../functions/cross_validate_spline.R")
```

## Grading {-}
The point value for each problem sub-part is indicated. Additionally, the presentation quality of the solution for each problem (as exemplified by the guidelines in Section 3 of the [preparing reports guide](https://github.com/Katsevich-Teaching/stat-471-fall-2021/blob/main/getting-started/preparing-reports.pdf) will be evaluated on a per-problem basis (e.g. in this homework, there are three problems). There are 100 points possible on this homework, 85 of which are for correctness and 15 of which are for presentation.

## Submission {-}

Compile your writeup to PDF and submit to [Gradescope](https://www.gradescope.com/courses/285259). 

\newpage 

# Case study: Bone mineral density (40 points for correctness; 10 points for presentation)

In this exercise, we will be looking at a data set (available [online](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/bone.data)) on spinal bone mineral density, a physiological indicator that increases during puberty when a child grows. 

Below is the [data description](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/bone.info.txt):

> "Relative spinal bone mineral density measurements on 261 North
> American adolescents. Each value is the difference in spnbmd
> taken on two consecutive visits, divided by the average. The age is
> the average age over the two visits."

> Variables:

> `idnum`:		identifies the child, and hence the repeat measurements

> `age`:		average age of child when measurements were taken

> `gender`:		male or female

> `spnbmd`:		Relative Spinal bone mineral density measurement

The goal is to learn about the typical trends of bone mineral density during puberty for boys and girls.

## Import (2 points)

- Using `readr`, import the data from the above URL into a tibble called `bmd`. Specify the column types using the `col_types` argument. 
- Print the imported tibble (no need to use `kable`). 

```{r, out.width="75%"}
bmd <- read_tsv("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/bone.data", col_types = "idfd")
print(bmd)
```

## Explore (10 points)

- To keep things simple, let's ignore the fact that we have repeated measurements on children. To this end, remote the `idnum` column from `bmd`.

```{r, out.width="75%"}
bmd <- bmd %>% 
  select(-idnum) #removes idnum column 
```

\newpage
- What is the number of boys and girls in this dataset (ignoring the fact that there are repeated measurements)? What are the median ages of these boys and girls?

```{r boy-girl, out.width="75%"}
boy_girl <- bmd %>%
  group_by(gender) %>%  #groups by gender
  summarise(count = n(), median_age = median(age)) #calculates observations and median age by gender

boy_girl  %>% 
  kable(format = "latex", row.names = NA,
      booktabs = TRUE, digits = 2,
      caption = "Boys and Girls in Sample") %>%
  kable_styling(position = "center")
```
**Their are 226 boys and 259 girls in the dataset. The median age for boys is 15.625 and the median age for girls is 15.35**

- Produce boxplots to compare the distributions of `spnbmd` and `age` between boys and girls (display these as two plots side by side, one for `spnbmd` and one for `age`). Are there apparent differences in either `spnbmd` or `age` between these two groups?
```{r gender-box, fig.align='center', out.width="75%", fig.cap = "Boxplots comparing relative spinal bone mineral density and age by gender" }
spnbmd_box <- bmd %>%
  ggplot() + 
  geom_boxplot(aes(x = gender, y = spnbmd)) + #creates boxplot
  labs(
    x = "Gender",
    y = "Relative Spinal Bone Mineral Density"
  ) + 
  ggtitle("Spinal Bone Mineral Density by Gender") + #adds title
  theme(plot.title = element_text(size = 10, face = "bold")) #formats title 

age_box <- bmd %>%
  ggplot() + 
  geom_boxplot(aes(x = gender, y = age)) + #creates boxplot
  labs(
    x = "Gender",
    y = "Age"
  ) + 
  ggtitle("Age by Gender") + #adds title 
  theme(plot.title = element_text(size = 10, face = "bold")) #formats title 
 
#plots grids next to each other 
plot_grid(spnbmd_box, age_box) 
```

**For spinal bone mineral density, the median is approximately equal for boys and girls. While both distibutions are skewed to the left, the male distribution is less skewed. Therefore females have a larger IQR. Also, the male distribution has observations that are below -.05, while the female distrubtion does not.**

**The median age for males is slightly higher than the median for females. The box plots suggest that the ages of males are distributed fairly normally, while the distribution of ages for females is slightly skewed to the right.**

**Overall, in Figure 1 there are not any differences by age that jump out to me** 

- Create a scatter plot of `spnbmd` (y axis) versus `age` (x axis), faceting by `gender`. What trends do you see in this data?

```{r gender-scatter, fig.align='center', out.width="75%", fig.cap = "Scatterplots by gender of relative spinal bone density measurement and age." }
bmd %>%
  ggplot() + 
  geom_point(aes(x = age, y = spnbmd)) + #creates scatter plot 
  labs(
    x = "Age",
    y = "Relative Spinal Bone Mineral Density"
  ) + 
  facet_wrap(~ gender) + # split into facets based on age
  ggtitle("Spinal Bone Mineral Density by Age") + #adds title
  theme(plot.title = element_text(size = 10, face = "bold")) #formats title 
```

**The is not one clear trend between age and relative spinal density. However, spinal bone density appears to peak at around the age of 12.5 for both males and females and then returns to around 0 0 between the ages of 17.5 and 20. Before age 12.5 bone mineral density appears to be increasing while after age 12.5, it appears to be on the decline. This trend is more clearly seen with females, where there appears to be less variance in the data**

## Model (15 points)

There are clearly some trends in this data, but they are somewhat hard to see given the substantial amount of variability. This is where splines come in handy.

### Split

To ensure unbiased assessment of predictive models, let's split the data before we start modeling it. 

- Split `bmd` into training (80%) and test (20%) sets, using the rows in `train_samples` below for training. Store these in tibbles called `bmd_train` and `bmd_test`, respectively.

```{r train-test}
set.seed(5) # seed set for reproducibility (DO NOT CHANGE)
n = nrow(bmd)
#randomly selects 80% of row numbers
train_samples = sample(1:n, round(0.8*n)) 
#selects training and test data 
bmd_train <- bmd %>% 
  filter(row_number() %in% train_samples)
bmd_test <- bmd %>% 
  filter(!(row_number() %in% train_samples))
```

### Tune

- Since the trends in `spnbmd` look somewhat different for boys than for girls, we might want to fit separate splines to these two groups. Separate `bmd_train` into `bmd_train_male` and `bmd_train_female`, and likewise for `bmd_test`.

```{r spline-gender}
bmd_train_male <- bmd_train %>% 
  filter(gender == "male") # filters for only males
bmd_train_female <- bmd_train %>% 
  filter(gender == "female") # filters for only females
bmd_test_male <- bmd_test %>% 
  filter(gender == "male") # filters for only males
bmd_test_female <- bmd_test %>% 
  filter(gender == "female") # filters for only females
```

- Using `cross_validate_spline` from Lecture 3, perform 10-fold cross-validation on `bmd_train_male` and `bmd_train_female`, trying degrees of freedom 1,2,...,15. Display the two resulting CV plots side by side.

```{r CV-plot, fig.align='center', out.width="85%", fig.cap = "Cross-validation plots for males and female training data. Plots indicate which degrees of freedom gives the lowest CV error"}
#source function from lecture 3 
source("~/Desktop/STAT471/stat-471-fall-2021/functions/cross_validate_spline.R")

#perform cross validation 
cv_male <- cross_validate_spline(bmd_train_male$age, 
                            bmd_train_male$spnbmd, 
                            nfolds = 10, #specify 10 fold 
                            df_values = 1:15) #specify 1-15 df 

cv_female <-  cross_validate_spline(bmd_train_female$age, 
                            bmd_train_female$spnbmd, 
                            nfolds = 10, #specify 10 fold 
                            df_values = 1:15) #specify 1-15 df
# plot 2 cv plots
plot_grid(cv_male$cv_plot, cv_female$cv_plot, 
          labels = c("Male", "Female"), 
          label_x = .45)
```

- What are the degrees of freedom values minimizing the CV curve for boys and girls, and what are the values obtained from the one standard error rule?

```{r}
#male CV
print(cv_male$df.min) #value minimizng the CV curve 
print(cv_male$df.1se) #value from one standard error rule

#female CV
print(cv_female$df.min) #value minimizng the CV curve 
print(cv_female$df.1se) #value from one standard error rule
```
**For males, 5 df minimizes the CV curve, but 3 df is obtained from the one standard error rule**
**For females, 4 df minimizes the CV curve, but 3 df is obtained from the one standard error rule**

- For the sake of simplicity, let's use the same degrees of freedom for males as well as females. Define `df.min` to be the maximum of the two `df.min` values for males and females, and define `df.1se` likewise. Add these two spline fits to the scatter plot of `spnbmd` (y axis) versus `age` (x axis), faceting by `gender`.
```{r spline-df-min-1se, fig.align='center', out.width = "85%", fig.cap = "Scatterplots by gender of relative spinal bone mineral density measurement and age with twi spline fits, one where the degrees of freedom minimizes the CV curve and one that is calculated using the one standard error rule"}
#max of df.min for males and females
df.min <- max(cv_male$df.min, cv_female$df.min) 
#max of df.1se for males and females 
df.1se <- max(cv_male$df.1se, cv_female$df.1se)
# Creates split fit 
bmd_train %>% 
  ggplot(aes(age, spnbmd)) + 
  geom_point() + 
  #plot spline using DF min
  geom_smooth(method = "lm", 
              formula = "y ~ splines::ns(x, df = df.min)",
              aes(colour = "Minimum DF"), se = FALSE) + 
  #plot spline using 1 SE rule
  geom_smooth(method = "lm", 
              formula = "y ~ splines::ns(x, df = df.1se)",
              aes(colour = "One SE Rule DF"), se = FALSE) + 
  scale_colour_manual(values = c("red", "blue")) +
  theme_bw() + theme(legend.title = element_blank()) +
    labs(
    x = "Age",
    y = "Relative Spinal Bone Mineral Density"
  ) +
  facet_wrap(~gender)
```

- Given our intuition for what growth curves look like, which of these two values of the degrees of freedom makes more sense?
**5 degrees of freedom makes sense because growth curves an exponential phase, transitonal phase, and plateau phase.Thus, the fit needs to grow as age increases until puberty, fall after puberty, and the plateau. When the 1 SE rule is used, females start to grow again after the age of 20, which does not seem logical.** 

### Final fit

- Using the degrees of freedom chosen above, fit final spline models to `bmd_train_male` and `bmd_train_female`. 
```{r final fit}
# Creates split fit for each gender with df.min degrees of freedom 
spline_fit_male = lm(spnbmd ~ splines::ns(age, df = df.min), 
                     data = bmd_train_male)
spline_fit_female = lm(spnbmd ~ splines::ns(age, df = df.min), 
                       data = bmd_train_female)
```

## Evaluate (6 points)

- Using the final models above, answer the following questions for boys and girls separately: What percent of the variation in `spnbmd` is explained by the spline fit in the training data? What is the training RMSE? What is the test RMSE? Print these three metrics in a nice table.
```{r male-female-fit}
#calculate male r^2, train error, test error
male_r_squared_train <- summary(spline_fit_male)$r.squared
male_train_RMSE <- bmd_train_male %>%
  mutate(fitted_values = spline_fit_male$fitted.values) %>%
  summarise(training_error = sqrt(mean((spnbmd - fitted_values)^2)))
#use model to predict on test data 
test_predict_male = predict(spline_fit_male, newdata = bmd_test_male)
male_test_RMSE <- bmd_test_male %>%
  summarise(test_error = sqrt(mean((spnbmd- test_predict_male)^2)))
#calculate female r^2, train error, test error
female_r_squared_train <- summary(spline_fit_female)$r.squared
female_train_RMSE <- bmd_train_female %>%
  mutate(fitted_values = spline_fit_female$fitted.values) %>%
  summarise(training_error = sqrt(mean((spnbmd - fitted_values)^2)))
#use model to predict on test data 
test_predict_female = predict(spline_fit_female, newdata = bmd_test_female)
female_test_RMSE <- bmd_test_female %>%
  summarise (test_error = sqrt(mean((spnbmd- test_predict_female)^2)))
#combine male and female into one table 
model_accuracy_table <- tibble("Gender" = c("Boys", "Girls"),
                               "R-Squared" = c(male_r_squared_train, female_r_squared_train),
                               "Training RMSE" = c(male_train_RMSE, female_train_RMSE), 
                               "Test RMSE" = c(male_test_RMSE, female_test_RMSE))
model_accuracy_table %>%
  kable(format = "latex", row.names = NA,
      booktabs = TRUE, digits = 2,
      caption = "Traning vs. Test RMSE for Male and Female Spline Fits") %>%
  kable_styling(position = "center")
```
**The male RMSE for the train date is higher than RMSE for test data, which suggests that the model for males does not overfit**
**The female RMSE for the train date is ~40% lower than RMSE for test data, which suggests that the model is overfitting**

## Interpret (7 points)

- Using the degrees of freedom chosen above, redo the scatter plot with the overlaid spline fits, this time without faceting in order to directly compare the spline fits for boys and girls. Instead of faceting, distinguish the genders by color.
```{r gender-spline-smooth, fig.align='center', fig.width = 9, fig.height = 5, fig.cap = "Scatterplot of relative spinal bone mineral density measurement and age with twi spline fits, one for males and one for females. Females peak before men. "}
bmd_train %>% 
  ggplot(aes(age, spnbmd, color = gender)) + 
  geom_point() + 
  # shows spline fit for boys and girls with df.min degrees of freedom
  geom_smooth(method = "lm", 
              formula = "y ~ splines::ns(x, df = df.min)", se = FALSE) + 
  #sets colors to blue and pink
  scale_colour_manual(values = c("blue", "magenta")) +
  theme_bw() + theme(legend.title = element_blank()) +
  labs(
    x = "Age",
    y = "Relative Spinal Bone Mineral Density"
  ) + 
  ggtitle("Bone Density by Age")  #adds title 
```

- The splines help us see the trend in the data much more clearly. Eyeballing these fitted curves, answer the following questions. At what ages (approximately) do boys and girls reach the peaks of their growth spurts? At what ages does growth largely level off for boys and girls? Do these seem in the right ballpark?

**Girls rech their peak growth spurts at around age 12 while boys reach the peaks of their growth spurts at around age 14.These numbers seem accurate because they are relatively close to the age each gender reaches puberty at**

# KNN and bias-variance tradeoff (45 points for correctness; 5 points for presentation)

## Setup: Apple farming {-}
You own a square apple orchard, measuring 200 meters on each side. You have planted trees in a grid ten meters apart from each other. Last apple season, you measured the yield of each tree in your orchard (in average apples per week). You noticed that the yield of the different trees seems to be higher in some places of the orchard and lower in others, perhaps due to differences in sunlight and soil fertility across the orchard.  

Unbeknownst to you, the yield $Y$ of the tree planted $X_1$ meters to the right and $X_2$ meters up from the bottom left-hand corner of the orchard has distribution
$$
Y = 50 + 0.001 X^2_1 + 0.001 X^2_2 + \epsilon, \quad \epsilon \sim N(0, \sigma^2), \quad \sigma = 4.
$$
The data you collected are as in Figure \@ref(fig:apple-data).
```{r apple-data, fig.align='center', fig.width = 4.5, fig.height = 4, out.width = "55%", fig.cap="Apple tree yield for each 10m by 10m block of the orchard in a given year.", echo = FALSE}
# problem parameters
orchard_width = 200                                 # orchard width, in meters
tree_distance = 10                                  # spacing between trees
n = (orchard_width/tree_distance + 1)^2             # total number of trees
sigma = 4                                           # noise level
f = function(X1, X2)(50 + 0.001*X1^2 + 0.001*X2^2)  # trend in yield

# training data (i.e. yields from last season)
data = crossing(X1 = seq(0,orchard_width, by = tree_distance), 
                X2 = seq(0,orchard_width, by = tree_distance)) %>%
  mutate(Yield = f(X1,X2) + rnorm(n, sd = sigma))

data %>% ggplot(aes(x = X1, y = X2, fill = Yield)) + 
  geom_tile() + 
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  coord_fixed() +
  labs(x = expr(X[1]),
       y = expr(X[2])) +
  theme_bw()
```

The underlying trend is depicted in Figure \@ref(fig:apple-trend), with the top right-hand corner of the orchard being more fruitful.
```{r apple-trend, fig.width = 4.5, fig.height = 4, out.width = "55%", fig.align='center', fig.cap="Underlying trend in apple yield for each 10m by 10m block of the orchard.", echo = FALSE}  
data %>% 
  mutate(Yield = f(X1,X2)) %>% 
  ggplot(aes(x = X1, y = X2, fill = Yield)) + 
  geom_tile() + 
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  coord_fixed() + 
  labs(x = expr(X[1]),
       y = expr(X[2])) +
  theme_bw()
```

## A simple rule to predict this season's yield (15 points)

This apple season is right around the corner, and you'd like to predict the yield of each tree. You come up with perhaps the simplest possible prediction rule: predict this year's yield for any given tree based on last year's yield from that same tree. Without doing any programming, answer the following questions:

- What is the expected training error of such a rule?
**Training error will be zero because the model has essentially fit every data point to itself. Therefore, we can perfectly predict yield for the train set.** 

- Averaged across all trees, what is the squared bias, variance, and ETE of this prediction rule? 
**The squared bias for this prediction rule is 0 since we can assumed that all trees stay in the same locaiton. Variance can be calculated since we know variance is equal to the irreducible error squared times the degress of freedom divided by the sample size. Irreducible error is equal to 16, and the degrees of freedom is equal to 2 because the model is only a slope and an intercept. The training sample size is equal to (200/10 + 1) squared, which gives us 441 trees. Therefore, the variance is equal to 32/441. The ETE is the sum of the squared bias, variance, and irreducible error. THus, ETE equals 0 + 32/441 + 16 = 7088/41. ** 

- Why is this not the best possible prediction rule? 
**This is not a possible prediciton rule because the rule tells us nothing about the underlying trend in the data. Thus, the model is generally useless in making predicitons. With the current predicition rule, we incorrectly assume that the random noise in the data set will remain idential from one year to the next. This does not help us explain large changes in yield. With the current model, we will predict that yield reamins constant forever.** 

## K-nearest neighbors regression (conceptual) (15 points)

As a second attempt to predict a yield for each tree, you average together last year's yields of the $K$ trees closest to it (including itself, and breaking ties randomly if necessary). So if you choose $K$ = 1, you get back the simple rule from the previous section. This more general rule is called *K-nearest neighbors (KNN) regression* (see ISLR p. 105). 

KNN is not a parametric model like linear or logistic regression, so it is a little harder to pin down its degrees of freedom. 

- What happens to the model complexity as $K$ increases? Why?
**As K incresaes, model complexity will also decrease. This happens because a small K results in a low training error and a high test error due to overfitting. Since the model is overfitting, variance will also be high, which means that model complexity is high. On the flip side, if K is low, the model underfits by reducing variance. Thus, when K is high, training accuracy and test accuracy are low. When a model underfits, it has high bias and low variance. Since bias decreases as mdoel complexity increases and variance increases as model complexity increases, model complexity must decrease as K increases.**

- The degrees of freedom for KNN is sometimes considered $n/K$, where $n$ is the training set size. Why might this be the case? [Hint: consider a situation where the data are clumped in groups of $K$.]
**When considering a situation were data is clumped into K number of groups, n/K neighbors would be represented in one parameter. Additionally, since the predicted value is equal to the average of K y-values and the sample point must be one of the included y-values, it makes sense that df = n/K. When K = n, the model will give just one prediction, since every data point in the cluster is averaged, which is the same as having just one parameter. When K = 1, the model outputs n different predicitons, which is the same as a model with n predictions.** 

- Conceptually, why might increasing $K$ tend to improve the prediction rule? What does this have to do with the bias-variance tradeoff?
**Larger K values tend to improve the prediction rule because they will have smoother decision boundaries, which means lower variance but increased bias. On the extreme side, if K = n, the model will predict the same y-value for every tree, which would give a variance of 0 but have high bias. As K increases, the training accuracy may decrease but the test accuaracy may be increasing. Thus, the model is overfitting less and less compared to a model with a smaller K. When we increase K, we are increasing variance, but decreasing bias.** 

- Conceptually, why might increasing $K$ tend to worsen the prediction rule? What does this have to do with the bias-variance tradeoff?
**Bias is the difference between the true label and the prediction, while variance is the expectation of the squared deviation of a random variable from its mean. Therefore, increasing K would reduce variance since predictions would become closer to each other. However, it would increase bias because the model now has a higher potential to overfit and predict values that are far from their true value. Therefore, when K increases, the training error will increase (which increases bias), but the test error may decrease at the same time (which decreases variance). ** 

## K-nearest neighbors regression (simulation) (15 points)

Now, we try KNN for several values of $K$. For each, we compute the bias, variance, and ETE for each value based on 50 resamples. The code for this simulation, provided for you below (see Rmd file; code omitted from PDF for brevity), results in Figure \@ref(fig:knn-bias-variance).
```{r, echo = FALSE, message = FALSE, warning = FALSE}
# set seed for reproducibility
set.seed(1)

# values of K
K_values = c(1,2,3,4,seq(5,50, by = 5))

# number of resamples
resamples = 50

# each entry in this list will consist of the KNN predictions for a resample
predictions_list = vector("list", resamples)

# iterate across resamples
for(resample in 1:resamples){
  # re-generate training data
  training_data = crossing(X1 = seq(0,orchard_width, by = tree_distance), 
                           X2 = seq(0,orchard_width, by = tree_distance)) %>%
    mutate(yield = f(X1, X2) + rnorm(n, sd = sigma))
  
  # calculate predictions based on KNN for each value of K
  predictions = matrix(0, n, length(K_values)) 
  for(K in K_values){
    knn_output = knn.reg(train = training_data %>% select(X1, X2),
                         test = training_data %>% select(X1, X2),
                         y = training_data %>% pull(yield),
                         k = K)
    predictions[,K_values == K] = knn_output$pred
  }
  
  # add predictions to the training data and store in predictions_list
  predictions_list[[resample]] = training_data %>% 
    select(X1, X2) %>%
    bind_cols(predictions %>% 
                as_tibble() %>%
                setNames(paste0('y_hat_', K_values))) %>%
    mutate(resample = resample)
}

# concatenate together predictions from all resamples 
training_results = do.call("rbind", predictions_list)
  
# compute bias, variance, and ETE for each test point
training_results_summary = training_results %>%
  mutate(true_fit = f(X1,X2)) %>%
  pivot_longer(-c(X1,X2,true_fit, resample), 
             names_to = "K",
             names_prefix = "y_hat_",
             names_transform = list(K = as.integer),
             values_to = "yhat") %>%
  group_by(K, X1, X2) %>% 
  summarise(bias = mean(yhat - true_fit),
            variance = var(yhat)) %>%
  ungroup()
  
# average across test points to get overall results
overall_results = training_results_summary %>% 
  group_by(K) %>%
  summarise(mean_sq_bias = mean(bias^2),
            mean_variance = mean(variance)) %>%
  mutate(expected_test_error = mean_sq_bias + mean_variance + sigma^2)
```

```{r knn-bias-variance, fig.width = 5, fig.height = 3, out.width = "100%", fig.align='center', fig.cap = "Bias-variance trade-off for KNN regression.", echo = FALSE}
# plot the bias, variance, and ETE
overall_results %>% 
  pivot_longer(-K, names_to = "metric", values_to = "Error") %>%
  mutate(metric = fct_recode(metric,
                             "Expected test error" = "expected_test_error",
                             "Mean squared bias" = "mean_sq_bias",
                             "Mean variance" = "mean_variance")) %>%
  ggplot(aes(x = K, y = Error, colour = metric)) + 
  geom_line() + 
  geom_point() + 
  theme_bw() + 
  theme(legend.title = element_blank())
```

- Based on Figure \@ref(fig:knn-bias-variance), what is the optimal value of `K`? 
```{r}
#identifies which K has the highest ETE 
overall_results$K[which.max(overall_results$expected_test_error)]
```
**The optimal value of K is 10 because ETE is minimized when K=10**

- We are used to the bias decreasing and the variance increasing when going from left to right in the plot. Here, the trend seems to be reversed. Why is this the case? 
**Degrees of freedom for KNN is equal to n/K. Thus, bias increases as K increases and variance decreases as K increases. When K increases, the predicted y-values get closer to each other because more points in the cluster are used to compute the y-value, which is an average of the K-nearest points. Therefore, when K increases, the training error will increases because the y-values are further away from their true values (which increases bias), but variance decreases because the predicted points become closer to each other. In fact, if K=n, mean variance will be 0 for the train data set**

- The squared bias has a strange bump between `K` = 1 and `K` = 5, increasing from `K` = 1 to `K` = 2 but then decreasing from `K` = 2 to `K` = 5. Why does this bump occur? [Hint: Think about the rectangular grid configuration of the trees. So for a given tree, the closest tree is itself, and then the next closest four trees are the ones that are one tree up, down, left, and right from it.]
**When K=2, one tree out of the four surrounding point is randomly selected to be averaged with y-value of the tree itself. If the surrounding data point that is most different from the point we are trying to pick is chosen, bias will increase. In constrastm when 3-5 data points are averaged to predict the y-value, the difference between the selected data point and the data points used for prediction decreases. As K goes from 2 to 5, more trees are averaged in, which decreases bias because it is less likely that one outliar can mess up your results.** 

- The data frame `training_results_summary` contains the bias and variance for every tree in the orchard, for every value of `K`. Which tree and which value of `K` gives the overall highest absolute bias? Does the sign of the bias make sense? Why do this particular tree and this particular value of `K` give us the largest absolute bias? 

```{r abs-bias-tree}
#identifies which row has the highest absolute sq bias 
highest_bias <- training_results_summary %>% 
  arrange(desc(abs(bias))) %>% 
  head(1) %>%
  select(K, X1, X2, bias)
#formats table 
highest_bias %>%  
  kable(format = "latex", row.names = NA,
      booktabs = TRUE, digits = 2,
      caption = "Tree with Overall Highest Absolute Sqaure Bias with Value of K") %>%
  kable_styling(position = "center")
```
**The tree located at (200, 200) when K = 50 gives the largest absolute bias. The bias is -20.6. The sign of the bias is negative because the predicted y-value was lower than the actual y-value. The true yields are higher than the predicted yields of our model. This particular tree has the largest bias because it is located at the top right corner of the plot. As X1 and Z2 increases, the yield also increases. Therefore, at the top right of the plot, the model is the most suspectible to underestimating the yield of the tree since KKN takes the 50 closest data points and averages then together. Since the top right tree should have the highest yield, it has the highest yield out of all 50 trees it is averaged with. K = 50 gives the highest bias because as K increases, we start to average in trees with even smaller yields, which increases absolute bias.**

- Redo the bias-variance plot above, this time putting `df = n/K` on the x-axis. What do we notice about the variance as a function of `df`? Derive a formula for the KNN variance and superimpose this formula onto the plot as a dashed curve. Do these two variance curves match? [Hint: To derive the KNN variance, focus first on the prediction of a single tree. Recall the facts that the variance of the sum of independent random variables is the sum of their variances, and that the variance of a constant multiple of a random variable is the square of that constant times its variance.]

```{r knn-bias-variance-df, fig.width = 5, fig.height = 3, out.width = "100%", fig.align='center', fig.cap = "Bias-variance trade-off for KNN regression where df = n/K.", echo = FALSE}
# plot the bias, variance, and ETE based on df = n/K
overall_results %>% 
  pivot_longer(-K, names_to = "metric", values_to = "Error") %>%
  mutate(df = n/K,      #add another for df = n/K
         metric = fct_recode(metric,
                             "Expected test error" = "expected_test_error",
                             "Mean squared bias" = "mean_sq_bias",
                             "Mean variance" = "mean_variance")) %>%
  ggplot(aes(x = df, y = Error, colour = metric)) + #set x equal to df 
  geom_line() + 
  geom_point() + 
  theme_bw() + 
  theme(legend.title = element_blank())
```
**Variance is a linear function of df (n/K), and variance increases as n increases or as K decreases. When predicting the yield for a single tree, you take the average of that tree's K closest neighbors. Since the variance of the sum of independent random variables is the sum of their variances, increases K reduces variance and decreasing n reduces variance. 1/K squared times the variance of the yields of the K-closest trees tells us that variance is equal to epsilon ove the K-closest trees. This is equal to K times 16 because we were told that epsilon has a mean of 0 and a standard deviation of 4. Our variance is given by 1/K-squared times K times 16 which is equal to 16/K. Since n = 441, the formula for variance of the graph above is equal to 16K/441 and equal to 16 times K/n and the equivalent to 16 tmes 1/df. Therefore, superimposing this curve on \@ref(fig:knn-bias-variance-df), it is clear that both curves match.**